program: experiments/torchtext_runner.py
command:
  - ${env}
  - python
  - ${program}
  - "--tokenizer"
  - bpe
  - "--experiment"
  - bpe
  - "--encoding"
  - index
  - "--model"
  - mlp
  - "--main"
  - wulczyn
  - "--aux"
  - oraby_factfeel
  - davidson
  - "--layers"
  - 1
  - "--datadir"
  - data/json/
  - "--results"
  - results/
  - "--save_model"
  - results/models/
  - "--shuffle"
  - True
  - "--cleaners"
  - username
  - hashtag
  - url
  - lower
  - "--metrics"
  - f1
  - accuracy
  - precision
  - recall
  - "--loss"
  - nlll
  - "--seed"
  - 42
  - "--batches_epoch"
  - 300
  - "--loss_weights"
  - "1.0,1.0,1.0"
method: bayes

metric:
  goal: maximize
  name: dev/f1-score

parameters:
  epochs:
    values: [50, 100, 200]
  batch_size:
    values: [16, 32, 64]
  embedding:
    values: [64, 100, 200, 300]
  hidden:
    values: ["64,64,64", "100,100,100", "200,200,200", "300,300,300"]
  shared:
    values: [64, 128, 256]
  learning_rate:
    distribution: uniform
    min: 0.00001
    max: 1.0
  dropout:
    distribution: uniform
    min: 0.0
    max: 0.5
  dataset_weights:
    values: ["0.6,0.2,0.2"]
  optimizer:
    values: ['adam', 'sgd', 'adamw', 'asgd']
  nonlinearity:
    values: ['tanh', 'relu']
