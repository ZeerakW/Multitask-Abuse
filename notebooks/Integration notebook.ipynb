{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.optim import Adam\n",
    "from mlearn.base import Field\n",
    "from mlearn.data import clean\n",
    "from mlearn.data import loaders\n",
    "from mlearn.modeling.multitask import OnehotLSTMClassifier\n",
    "from mlearn.data.dataset import GeneralDataset\n",
    "from mlearn.utils.early_stopping import EarlyStopping\n",
    "from mlearn.utils.train import process_and_batch, train_mtl_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = clean.Cleaner(processes = ['lower', 'url', 'hashtag'])\n",
    "pr = clean.Preprocessors(liwc_dir = '~/PhD/projects/active/MTL_abuse/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Davidson et al. (train): 24783it [01:36, 255.52it/s]\n"
     ]
    }
   ],
   "source": [
    "## Slow version\n",
    "davidson = loaders.davidson(cleaners = cl, data_path = '~/PhD/projects/active/MTL_abuse/data/', length = 200,\n",
    "                            label_processor = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Hoover et al. (train): 34987it [02:18, 253.34it/s]\n"
     ]
    }
   ],
   "source": [
    "## Slow version\n",
    "hoover = loaders.hoover(cleaners = cl, data_path = '~/PhD/projects/active/MTL_abuse/data/', length = 200,\n",
    "                        preprocessor = pr.word_token, label_processor = lambda x: x.split()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocabulary: 100%|██████████| 24783/24783 [00:00<00:00, 181417.05it/s]\n",
      "Encoding vocabulary: 100%|██████████| 53683/53683 [00:00<00:00, 1095933.48it/s]\n",
      "Encode label vocab: 100%|██████████| 3/3 [00:00<00:00, 9467.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Davidson\n",
    "davidson.build_token_vocab(davidson.data)\n",
    "davidson.build_label_vocab(davidson.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocabulary: 100%|██████████| 27989/27989 [00:00<00:00, 147005.56it/s]\n",
      "Encoding vocabulary: 100%|██████████| 42590/42590 [00:00<00:00, 1031590.72it/s]\n",
      "Encode label vocab: 100%|██████████| 11/11 [00:00<00:00, 20876.63it/s]\n"
     ]
    }
   ],
   "source": [
    "hoover.build_token_vocab(hoover.data)\n",
    "hoover.build_label_vocab(hoover.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hoover.ltoi)\n",
    "print(davidson.ltoi)\n",
    "print(hoover.vocab_size())\n",
    "print(davidson.vocab_size())\n",
    "print(hoover.ltoi)\n",
    "print(davidson.ltoi)\n",
    "print(hoover.data[0].__dict__)\n",
    "print(davidson.data[0].__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OnehotLSTMClassifier(input_dims = [int(hoover.vocab_size()), int(davidson.vocab_size())], shared_dim = 150,\n",
    "                          hidden_dims = [128, 128], output_dims = [hoover.label_count(), davidson.label_count()],\n",
    "                          no_layers = 1, dropout = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, [hoover, davidson], 'results/', optimizer, dev_data = hoover.dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, training_datasets, save_path, optimizer,\n",
    "                batch_size=64, epochs=2, dev_data=None, clip=None,\n",
    "                dev_task_id=0,\n",
    "                patience=10, batches_per_epoch=None, shuffle_data=True,\n",
    "                loss_weights=None, loss_func = None):\n",
    "    \"\"\"\n",
    "    Trains a model\n",
    "    :param model:\n",
    "    :param training_datasets: list of tuples containing dense matrices\n",
    "    :param save_path: path to save trained model to\n",
    "    :param optimizer: Pytorch optimizer to train model\n",
    "    :param batch_size: Training batch size\n",
    "    :param patience: Number of epochs to observe non-improving dev performance\n",
    "    before early stopping\n",
    "    :param epochs: Maximum number of epochs (if no early stopping)\n",
    "    :param dev_data: tuple (x, y) of development data\n",
    "    :param dev_task_id: Task ID for task to use for early stopping, in case of\n",
    "    multitask learning\n",
    "    :param clip: use gradient clipping\n",
    "    :param batches_per_epoch: set fixed number of batches per epoch. If\n",
    "    None, an epoch consists of all training examples\n",
    "    :param shuffle_data: whether to shuffle data at training\n",
    "    :param loss_weights: array or list of floats. When using multiple\n",
    "    input/output functions, these weights determine relative task importance\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if loss_weights is None:\n",
    "        loss_weights = np.ones(len(training_datasets))\n",
    "\n",
    "    if batches_per_epoch is None:\n",
    "        batches_per_epoch = sum([len(dataset) * batch_size for dataset\n",
    "                                 in training_datasets]) // batch_size\n",
    "    if patience > 0:\n",
    "        early_stopping = EarlyStopping(save_path, patience,\n",
    "                                       low_is_good=False)\n",
    "        \n",
    "    batchers, extractors = [], []\n",
    "\n",
    "    for training_data in training_datasets:\n",
    "        batches = process_and_batch(training_data, training_data.data, batch_size, 'label')\n",
    "        batchers.append(batches)\n",
    "        \n",
    "        #batch, extractor = batch_data(training_data)\n",
    "        #batchers.append(batch)\n",
    "        #extractors.append(extractor)\n",
    "        \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        epoch_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for b in range(batches_per_epoch):\n",
    "            task_id = np.random.choice(range(len(training_datasets)), p = [0.8, 0.2]) # set probability for each task\n",
    "            batcher = batchers[task_id]\n",
    "            X, y = next(iter(batcher))\n",
    "            \n",
    "            # Do model training\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            breakpoint()\n",
    "            \n",
    "            preds = model(X, task_id)\n",
    "            loss = loss_func(preds, y) * loss_weight[task_id]\n",
    "            loss.backwards()\n",
    "            \n",
    "            if clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm(model.parameters(), clip)  # Prevent exploding gradients\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.data.item().cpu()\n",
    "            \n",
    "            print(\"Epoch train loss:\", np.array(epoch_cwi_loss).mean())\n",
    "\n",
    "        if dev_data is not None:\n",
    "            batch, extractor = batch_data(dev_data, len(dev_data))\n",
    "            X_dev, y_dev = next(iter(extractor))\n",
    "            score, corr, _ = eval_model(model, X_dev, y_dev,\n",
    "                                        task_id=dev_task_id,\n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "            if early_stopping is not None and early_stopping(model, score):\n",
    "                early_stopping.set_best_state(model)\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
