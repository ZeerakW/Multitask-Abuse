{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "sys.path.extend(['/Users/zeerakw/Documents/PhD/projects/active/Multitask-abuse/src'])\n",
    "\n",
    "from gen.shared.data import GeneralDataset\n",
    "from gen.shared.batching import Batch, BatchExtractor\n",
    "from gen.shared.base import Field\n",
    "from gen.neural import RNNClassifier\n",
    "from gen.shared.clean import Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(dataset, batch_size):\n",
    "    batched = Batch(batch_size, dataset)\n",
    "    batched.create_batches()\n",
    "    extractor = BatchExtractor('encoded', 'label', batched)\n",
    "    return (batched, extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading Davidson et al. (train): 887it [00:00, 46776.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Davidson\n",
    "text_field = Field('text', train = True, label = False, ignore = False, ix = 6, cname = 'text')\n",
    "label_field = Field('label', train = False, label = True, cname = 'label', ignore = False, ix = 5)\n",
    "ignore_field = Field('ignore', train = False, label = False, cname = 'ignore', ignore = True)\n",
    "\n",
    "davidson_fields = [ignore_field, ignore_field, ignore_field, ignore_field, ignore_field, label_field, text_field]\n",
    "\n",
    "davidson = GeneralDataset(data_dir = '~/PhD/projects/active/Generalisable_abuse/data/',\n",
    "                          ftype = 'csv', fields = davidson_fields, train = 'davidson_train.csv', dev = None,\n",
    "                          test = None, train_labels = None, tokenizer = lambda x: x.split(),\n",
    "                          lower = True, preprocessor = None, transformations = None,\n",
    "                          label_processor = None, sep = ',', name = 'Davidson et al.')\n",
    "davidson.load('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading Garcia et al. (train): 1914it [00:00, 22566.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Garcia\n",
    "t_field = Field('text', train = True, label = False, ignore = False, ix = 5, cname = 'text')\n",
    "l_field = Field('label', train = False, label = True, cname = 'label', ignore = False, ix = 4)\n",
    "\n",
    "garcia_fields = [ignore_field, ignore_field, ignore_field, l_field, t_field]\n",
    "\n",
    "garcia = GeneralDataset(data_dir = '~/PhD/projects/active/Generalisable_abuse/data/',\n",
    "                        ftype = 'tsv', fields = garcia_fields, train = 'garcia_stormfront_train.tsv', dev = None,\n",
    "                        test = None, train_labels = None, tokenizer = lambda x: x.split(),\n",
    "                        lower = True, preprocessor = None, transformations = None,\n",
    "                        label_processor = lambda x: x[0], sep = '\\t', name = 'Garcia et al.')\n",
    "garcia.load('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocabulary: 100%|██████████| 709/709 [00:00<00:00, 41646.99it/s]\n",
      "Encoding data: 100%|██████████| 709/709 [00:00<00:00, 866.72it/s]\n",
      "Encoding data: 0it [00:00, ?it/s]\n",
      "Encoding data: 100%|██████████| 88/88 [00:00<00:00, 553.25it/s]\n"
     ]
    }
   ],
   "source": [
    "train, dev, test = davidson.split(davidson.data, [0.8, 0.1, 0.1])\n",
    "davidson.build_token_vocab(train)\n",
    "davidson.build_label_vocab(train)\n",
    "davidson.process_labels(train)\n",
    "\n",
    "davidson.process_labels(dev)\n",
    "\n",
    "davidson_train = davidson.encode(train, onehot = True)\n",
    "davidson_dev = davidson.encode(dev, onehot = True)\n",
    "davidson_test = davidson.encode(test, onehot = True)\n",
    "davidson.process_labels(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocabulary: 100%|██████████| 1531/1531 [00:00<00:00, 124693.76it/s]\n",
      "Encoding data: 100%|██████████| 1531/1531 [00:24<00:00, 62.13it/s]\n",
      "Encoding data: 0it [00:00, ?it/s]\n",
      "Encoding data: 100%|██████████| 191/191 [00:03<00:00, 62.08it/s]\n"
     ]
    }
   ],
   "source": [
    "garcia_train, garcia_dev, garcia_test = garcia.split(garcia.data, [0.8, 0.1, 0.1])\n",
    "\n",
    "garcia.build_token_vocab(garcia_train)\n",
    "garcia.build_label_vocab(garcia_train)\n",
    "garcia.process_labels(garcia_train)\n",
    "\n",
    "garcia.process_labels(garcia_dev)\n",
    "\n",
    "garcia_train = garcia.encode(garcia_train, onehot = True)\n",
    "garcia_dev = garcia.encode(garcia_dev, onehot = True)\n",
    "garcia_test = garcia.encode(garcia_test, onehot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, training_datasets, save_path, optimizer,\n",
    "                batch_size=64, epochs=30, dev_data=None, clip=None,\n",
    "                dev_task_id=0,\n",
    "                patience=10, batches_per_epoch=None, shuffle_data=True,\n",
    "                loss_weights=None, loss_func = None):\n",
    "    \"\"\"\n",
    "    Trains a model\n",
    "    :param model:\n",
    "    :param training_datasets: list of tuples containing dense matrices\n",
    "    :param save_path: path to save trained model to\n",
    "    :param optimizer: Pytorch optimizer to train model\n",
    "    :param batch_size: Training batch size\n",
    "    :param patience: Number of epochs to observe non-improving dev performance\n",
    "    before early stopping\n",
    "    :param epochs: Maximum number of epochs (if no early stopping)\n",
    "    :param dev_data: tuple (x, y) of development data\n",
    "    :param dev_task_id: Task ID for task to use for early stopping, in case of\n",
    "    multitask learning\n",
    "    :param clip: use gradient clipping\n",
    "    :param batches_per_epoch: set fixed number of batches per epoch. If\n",
    "    None, an epoch consists of all training examples\n",
    "    :param shuffle_data: whether to shuffle data at training\n",
    "    :param loss_weights: array or list of floats. When using multiple\n",
    "    input/output functions, these weights determine relative task importance\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if loss_weights is None:\n",
    "        loss_weights = np.ones(len(training_datasets))\n",
    "\n",
    "    if batches_per_epoch is None:\n",
    "        batches_per_epoch = sum([len(dataset[0]) * batch_size for dataset\n",
    "                                 in training_datasets]) // batch_size\n",
    "    if patience > 0:\n",
    "        early_stopping = EarlyStopping(save_path, patience,\n",
    "                                       low_is_good=not model.binary)  # What is model.binary?\n",
    "        \n",
    "    batchers, extractors = [], []\n",
    "    \n",
    "    for training_data in training_datasets:\n",
    "        batch, extractor = batch_data(training_data)\n",
    "        batchers.append(batch)\n",
    "        extractors.append(extractor)\n",
    "        \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        epoch_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for b in range(batches_per_epoch):\n",
    "            task_id = np.random.choice(range(len(training_datasets)), p = [0.8, 0.2]) # set probability for each task\n",
    "            batcher = extractors[task_id]\n",
    "            X, y = next(iter(batcher))\n",
    "            \n",
    "            # Do model training\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   preds = model(X, task_id)\n",
    "            loss = loss_func(preds, y) * loss_weight[task_id]\n",
    "            loss.backwards()\n",
    "            \n",
    "            if clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm(model.parameters(), clip)  # Prevent exploding gradients\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.cpu()\n",
    "            \n",
    "            print(\"Epoch train loss:\", np.array(epoch_cwi_loss).mean())\n",
    "\n",
    "        if dev_data is not None:\n",
    "            batch, extractor = batch_data(dev_data, len(dev_data))\n",
    "            X_dev, y_dev = next(iter(extractor))\n",
    "            score, corr, _ = eval_model(model, X_dev, y_dev,\n",
    "                                        task_id=dev_task_id,\n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "            if early_stopping is not None and early_stopping(model, score):\n",
    "                early_stopping.set_best_state(model)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JOACHIMS_train_model(model, training_datasets, save_path, optimizer,\n",
    "                batch_size=64, epochs=30, dev_data=None, clip=None,\n",
    "                dev_task_id=0,\n",
    "                patience=10, batches_per_epoch=None, shuffle_data=True,\n",
    "                loss_weights=None, loss_decay_aux=True):\n",
    "    \"\"\"\n",
    "    Trains a model\n",
    "    :param model:\n",
    "    :param training_datasets: list of tuples containing dense matrices\n",
    "    :param save_path: path to save trained model to\n",
    "    :param optimizer: Pytorch optimizer to train model\n",
    "    :param batch_size: Training batch size\n",
    "    :param patience: Number of epochs to observe non-improving dev performance\n",
    "    before early stopping\n",
    "    :param epochs: Maximum number of epochs (if no early stopping)\n",
    "    :param dev_data: tuple (x, y) of development data\n",
    "    :param dev_task_id: Task ID for task to use for early stopping, in case of\n",
    "    multitask learning\n",
    "    :param clip: use gradient clipping\n",
    "    :param batches_per_epoch: set fixed number of batches per epoch. If\n",
    "    None, an epoch consists of all training examples\n",
    "    :param shuffle_data: whether to shuffle data at training\n",
    "    :param loss_weights: array or list of floats. When using multiple\n",
    "    input/output functions, these weights determine relative task importance\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if loss_weights is None:\n",
    "        loss_weights = np.ones(len(training_datasets))\n",
    "\n",
    "    if batches_per_epoch is None:\n",
    "        batches_per_epoch = sum([len(dataset[0]) * batch_size for dataset\n",
    "                                 in training_datasets]) // batch_size\n",
    "    batchers = []\n",
    "\n",
    "    early_stopping = None\n",
    "    if patience > 0:\n",
    "        early_stopping = EarlyStopping(save_path, patience,\n",
    "                                       low_is_good=not model.binary)  # ZW: What is model.binary?\n",
    "\n",
    "    for training_dataset in training_datasets:\n",
    "        \n",
    "        X, y = training_dataset\n",
    "        if shuffle_data:  # ZW: Why shuffling it out here instead of within the epoch loop?\n",
    "            X, y = shuffle(X, y)\n",
    "\n",
    "        batcher = Batcher(len(X), batch_size)\n",
    "        batchers.append(batcher)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        if loss_decay_aux:\n",
    "            loss_weights[1:] = loss_weights[1:] * 0.9\n",
    "        epoch_loss = 0\n",
    "        epoch_cwi_loss = []  # ZW: Why this additional loss?\n",
    "        epoch_data_size = 0\n",
    "        for b in range(batches_per_epoch):\n",
    "            task_id = random.choice(range(len(training_datasets)))\n",
    "            batcher = batchers[task_id]\n",
    "            X, y = training_datasets[task_id]\n",
    "            X = torch.tensor(X).float()\n",
    "            y = torch.tensor(y).float()\n",
    "            size, start, end = batcher.next_loop()\n",
    "            d, gold = Variable(X[start:end]), y[start:end]\n",
    "            model.train() # ZW: What does this do?\n",
    "            optimizer.zero_grad()   # Why setting zero gradient for the optimizer?\n",
    "            logits = model(d, input_task_id=task_id)\n",
    "\n",
    "            logits = logits.view([size, 1])  # Why?\n",
    "            if model.binary:\n",
    "                loss = torch.nn.functional.binary_cross_entropy(logits, gold)\n",
    "            else:\n",
    "                loss = (logits - gold).pow(2).mean()\n",
    "            loss = loss * loss_weights[task_id]\n",
    "            epoch_cwi_loss.append(loss.data.numpy())  # ?\n",
    "            loss.backward()\n",
    "\n",
    "            epoch_loss += loss.cpu()\n",
    "            epoch_data_size += size\n",
    "\n",
    "            if clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Epoch train loss:\", np.array(epoch_cwi_loss).mean())\n",
    "\n",
    "        if dev_data is not None:\n",
    "            X_dev, y_dev = dev_data\n",
    "            score, corr, _ = eval_model(model, X_dev, y_dev,\n",
    "                                        task_id=dev_task_id,\n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "            if early_stopping is not None and early_stopping(model, score):\n",
    "                early_stopping.set_best_state(model)\n",
    "                break\n",
    "\n",
    "    if early_stopping is not None:\n",
    "        early_stopping.set_best_state(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
